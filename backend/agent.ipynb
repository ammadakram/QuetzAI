{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3968f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r data/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95b3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "# used to load the pdfs \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# used to create the retrieval tool\n",
    "from langchain.agents import tool\n",
    "\n",
    "# used to create the memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# used to create the prompt template\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.agents.openai_functions_agent.base import create_openai_functions_agent\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "# used to create the agent executor\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Type, List, Union\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.callbacks.manager import (AsyncCallbackManagerForToolRun, CallbackManagerForToolRun)\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import AgentType, initialize_agent, Tool, AgentExecutor, AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e18f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your API key here\n",
    "openai_api_key = open(\"API_key.txt\").read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"SERPAPI_API_KEY\"] = '4bde37c74068633153b825cb3ff392ee3a6697a630674d5abb5d55be50f58a49'\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDjYdIvnFq1Uw2DW7C5sj8fbXCl9HfF89c\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"74e790ea0cfaf4154\"\n",
    "memory_key = \"history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a75f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0, openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df8336",
   "metadata": {},
   "source": [
    "# Hashing Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d57849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures robustness when saving indices. Always ensures each doc or combination of merged docs is assigned a unique path name\n",
    "def hash_string(input_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Hashes a given string using SHA-256 algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        input_string (str): The string to be hashed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the string to bytes before hashing\n",
    "    input_bytes = input_string.encode('utf-8')\n",
    "    \n",
    "    # Create a SHA-256 hash object\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    \n",
    "    # Update the hash object with the input bytes\n",
    "    sha256_hash.update(input_bytes)\n",
    "    \n",
    "    # Get the hexadecimal digest of the hash\n",
    "    hashed_value = sha256_hash.hexdigest()\n",
    "    \n",
    "    return hashed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce2bb1",
   "metadata": {},
   "source": [
    "# VectorBase Logic (File Upload Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403b4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns page by page doc in pyPDF wrapper \n",
    "def load_document(doc_path: str = \"\"):\n",
    "    \"\"\"\n",
    "    Returns the pyPDF wrapped doc.\n",
    "    \n",
    "    parameters:\n",
    "    doc_path: this is the path to the uploaded document\n",
    "    \"\"\"\n",
    "    \n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    doc = loader.load()\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d194c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an OpenAI embeddings object\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfda496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function simply checks whether the file_name is present at folder_path\n",
    "def check_file_exists(folder_path, file_name):\n",
    "    \"\"\"\n",
    "    Checks whether file_name is present at folder_path or not.\n",
    "    \n",
    "    parameters:\n",
    "    folder_path: this will be the folder which stores the db info for a PARTICULAR user.\n",
    "    file_name: just the file name of the particular file at hand (hashed value). THIS IS NOT THE WHOLE file path.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    return os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2da9a",
   "metadata": {},
   "source": [
    "### PLEASE CHANGE THE SAVED_DB_ROOT VARIABLE HERE ACCORDING TO YOUR PATHS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3d8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_retriever(doc_paths: List[str] = None, chunk_size: int = 1000, overlap: int = 50):\n",
    "    \"\"\"\n",
    "    doc_paths: List of paths to the user uploaded documents.\n",
    "    chunk_size: Maximum chunk size to which doc is broken into.\n",
    "    overlap: The # of overlapping tokens between chunks (helps with retaining some context between chunks).\n",
    "    \n",
    "    Chunkifies the docs, creates embeddings, uploads embeddings to vector database. Saves and returns retriever.\n",
    "    In case doc was already found at the user path, simply return the old retriever object.\n",
    "    \n",
    "    Whenever len(doc_paths) > 1, assume user wants to merge the docs in the database. \n",
    "    db1.merge_from(db2) collapses db2 into the index of db1 so there is no need to store db2 separately.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not doc_paths or len(doc_paths) < 1:\n",
    "        raise ValueError(\"Must give at least one document\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    \n",
    "    # extract the document names from the doc paths provided by the user \n",
    "    doc_names = [doc.split('/')[-1].split('.')[0] for doc in doc_paths]\n",
    "    \n",
    "    \n",
    "    # CHANGE THIS TO YOUR ROOT (the path to the folder where db info for ALL USERS is stored)\n",
    "    saved_db_root = 'data/indices' \n",
    "    \n",
    "    # Combine all names for hashing\n",
    "    saved_db_name = ('_').join(doc_names)\n",
    "    \n",
    "    # This is the name of the db index for the current doc paths supplied by the user\n",
    "    saved_db_value = hash_string(saved_db_name)\n",
    "    \n",
    "    # This is the path to the db file where we save the vectorstore index created by this function call (same as saved_db_value but this is the full path with root included)\n",
    "    saved_db_path = os.path.join(saved_db_root, saved_db_value)\n",
    "    \n",
    "    # If this saved_db_name already exists at saved_db_value, then simply fetch the db from the index and return it\n",
    "    if check_file_exists(saved_db_root, saved_db_value):\n",
    "        return FAISS.load_local(saved_db_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # If this is a new set of documents, then create and merge the dbs and then save\n",
    "    \n",
    "    # Since all merged dbs need to merge onto some existing db, our db_base will be that special db (its the first doc from doc_paths for simplicity)\n",
    "    db_base = None \n",
    "    \n",
    "    for i, doc_path in enumerate(doc_paths):        \n",
    "        doc = load_document(doc_path)\n",
    "        texts = text_splitter.split_documents(doc)\n",
    "        db = FAISS.from_documents(texts, embeddings) \n",
    "        \n",
    "        # Set the first db to db_base. Otherwise, merge current db with db_base \n",
    "        if i == 0:\n",
    "            db_base = db\n",
    "        else:\n",
    "            # Merge current db object with the base db object.\n",
    "            db_base.merge_from(db)\n",
    "        \n",
    "    # Save the db object every time a new one is created\n",
    "    db_base.save_local(saved_db_path)\n",
    "    \n",
    "    return db_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6f929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_paths = [\"data/google_earning_call.pdf\", \"data/tesla_earning_call.pdf\", \"data/G04_RA1.pdf\"]\n",
    "\n",
    "db = establish_retriever(doc_paths) \n",
    "retriever = db.as_retriever() # This gives us the retriever object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecdc600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='tributed systems to generate translations. However,\\nthe authors propose an alternative strategy where\\nthey only have to contact one system. This opti-\\nmization is achieved by maintaining pre-calculated\\nprobabilities for various phrases rather than rely-\\ning on raw data. Their approach demonstrates an\\nefficiency advantage.\\n4.2 Stupid Backoff\\nStupid backoff does not generate probabilities but\\nrather scores. This could be a potential limitation\\nbecause many applications using language models\\nrely on normalized probabilities to make decisions.\\nWe suggest the authors discuss this as a limitation\\nand propose a workaround for users who want to\\nuse probabilities rather than scores.\\n4.3 Generation of n-grams\\nThe text mentions that sharding based solely on\\nthe first word of n-grams can lead to imbalanced\\nshards, impacting runtime. The authors need to\\nmention this as a potential limitation and elaborate\\non how they fixed it (using the first two words for\\nhashing).\\n4.4 Distributed Application', metadata={'source': 'data/G04_RA1.pdf', 'page': 2}),\n",
       " Document(page_content='showcases the authors’ ingenuity in addressing\\nefficiency and overhead concerns which is a major\\nstrength of the paper.\\nSecondly, the authors’ ability to clearly artic-\\nulate the challenges they faced when training a\\nmodel at this scale (up to a trillion tokens) sets the\\nstage perfectly for their proposed Stupid Backoff\\nmethod. For example, the number of additional\\nsteps required for the distributed training of the\\nKneser-Ney algorithm (Table 1, page 5) already\\nmakes it a daunting option compared to the much\\nsimpler Stupid Backoff method.\\nThirdly, the authors’ ability to go beyond just\\nstating facts or equations adds more meaning to\\ntheir work. We would like to cite two distinct ex-\\namples of this:\\n•When presenting the stupid backoff model for-\\nmally (Brants et al., 2007) (equation 5, page\\n2), the authors articulate that this equation has\\nshifted the paradigm outside the domain of\\nlegal probabilities due to the lack of any nor-\\nmalization scheme. Furthermore, they explain', metadata={'source': 'data/G04_RA1.pdf', 'page': 1}),\n",
       " Document(page_content='it. Although the task of evaluating language\\nmodels is an entirely different research niche,\\nit would have added immensely to the paper\\nhad the authors dedicated a subsection to\\ndescribe the massive challenge that language\\nmodel evaluation actually is rather than just\\nciting shortcomings of Perplexity and sticking\\nwith it.\\nDespite these weaknesses, the significance of\\nthe paper is undeniable. The ability of Stupid\\nBackoff to outperform the SOTA Kneser-\\nNey smoothing alluded to the future of NLP\\nand machine learning in general where large\\nmodels and even larger datasets would reign\\nsupreme. The AlexNet paper (Krizhevsky\\net al., 2012) would also use a distributed train-\\ning protocol and a cheaper activation (ReLU)\\nto revolutionize Computer Vision and kick-\\nstart the deep learning era as we know it.\\n3 A List of Pressing Questions\\nUpon reading the paper, a myriad questions came\\nto us, answering which we think would lead to\\ngreater inroads in the topic and perhaps enlighten', metadata={'source': 'data/G04_RA1.pdf', 'page': 1}),\n",
       " Document(page_content='allow for relative frequencies to be calculated\\nwithin the shards.\\nTo integrate the language model into a dis-\\ntributed training system, the authors propose a new\\ndecoder architecture that utilises batch processing\\nto improve efficiency. The decoder queues n-gram\\nrequests and sends them to servers in batches to\\nminimise network overhead. The results of the ex-\\nperiments conducted on several models with vary-\\ning token inputs show that increasing the amount of\\ntraining data leads to almost linear improvements\\nin BLEU score.\\n2 An Overview of Strengths and\\nWeaknesses\\n2.1 Strengths\\nThe paper has a series of strengths that span across\\nseveral fronts. Notably, these strengths manifest in\\nthe originality of ideas, articulation of challenges\\nand a deeper presentation of facts and figures\\nbeyond the cursory layer of presentation.\\nFirstly, the authors demonstrate commendable\\noriginality in introducing the Stupid Backoff as\\na less expensive smoothing method. Moreover,', metadata={'source': 'data/G04_RA1.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('does the proposed Stupid Backoff work well?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec5085",
   "metadata": {},
   "source": [
    "# Creating Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af90f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the retrieval tool that searches the documents and fetches chunks most similar to user query using FAISS.\n",
    "@tool\n",
    "def retriever_tool(query:str = None):\n",
    "    \"Searches and returns relevant documents to user queries regarding the uploaded document\"\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return docs\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6384ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an arithmetic tool which simply performs any calculation the user might want ot perform. \n",
    "@tool \n",
    "def math_tool(query):\n",
    "    \"Takes in a arithmetic equation as query and solves the equation in a step-by-step manner.\"\n",
    "    llm_math_chain = LLMMathChain.from_llm(llm, verbose=True)\n",
    "    response = llm_math_chain.run(query)\n",
    "    \n",
    "    return response\n",
    "\n",
    "tools.append(math_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed8948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713caaa",
   "metadata": {},
   "source": [
    "# Agent Logic (Generation Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff198a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vital to retain memory since that is what enables the back-forth convo\n",
    "memory = ConversationBufferMemory(memory_key=memory_key, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d649b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom system prompt\n",
    "sys_prompt = \"\"\"\n",
    "You are an agent who has a second brain, namely its retrieval tool.\\\n",
    "You make use of this tool along with your search and math tools to answer user queries.\\\n",
    "\n",
    "Whenever given a query, follow this protocol to answer:\\\n",
    "1. Always call your retrieval tool to fetch the data relevant to answering the user query.\\\n",
    "2. Identify wether you need to use the search tool or not. ONLY USE THE SEARCH TOOL IF NO DATA IS RETURNED BY RETRIEVAL TOOL.\\\n",
    "3. Augment the retrieved results and answer the user query.\n",
    "\n",
    "Follow these instructions to format your final response:\n",
    "1. Be concise. \n",
    "2. Follow the protocol mentioned above AT ALL TIMES.\n",
    "3. Before using the search tool as the user explicitly \"NO RESULTS FOUND. WANT ME TO SEARCH THE WEB?\"\\\n",
    "if the reply is a yes only then use the search tool otherwise do not use it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7277329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are simply wrapping the prompts to feed into the model\n",
    "system_message = SystemMessage(\n",
    "        content=(f\"{sys_prompt}\")\n",
    ")\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=system_message,\n",
    "        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5cf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define the AGENT\n",
    "agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68cc301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where everything is wrapped together to form our RAG agent. RAG is just one capability enabled by our retrieval tool. Adding more tools gives more abilities\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eada7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function passes user queries to the model and returns the inferred response\n",
    "def infer(query):\n",
    "    agent_input = {'input':query}\n",
    "    result = agent_executor(agent_input)\n",
    "    \n",
    "    return result['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480dd362",
   "metadata": {},
   "source": [
    "# Example Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60246e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khaquansheikh/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retriever_tool` with `{'query': 'effect of staff cuts on Google severance costs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='Severance and office space exit charges are included within our consolidated statements of income as follows (in \\nmillions):\\nQuarter Ended December 31, 2023 Year Ended December 31, 2023\\nSeverance \\nand Related Office Space TotalSeverance \\nand Related Office Space Total\\nCost of revenues $ 2 $ 235 $ 237 $ 479 $ 481 $ 960 \\nResearch and development  1  602  603 848  870  1,718 \\nSales and marketing  3  198  201 497  257  754 \\nGeneral and administrative  1  161  162 264  237  501 \\nTotal charges $ 7 $ 1,196 $ 1,203 $ 2,088 $ 1,845 $ 3,933 \\nFor segment reporting, the substantial majority of these charges are included within Alphabet-level activities in our \\nsegment result s.\\nChange in Useful Lives of Our Server and Network Equipment\\nIn January 2023, we completed an assessment of the useful lives of our servers and network equipment and \\nadjusted the estimated useful life of our servers from four years  to six years  and the estimated useful life of certain', metadata={'source': 'data/google_earning_call.pdf', 'page': 2}), Document(page_content='Total income from operations $ 18,160 $ 23,697 \\n(1)Formerly “corporate costs, unallocated.” In addition to the costs included in Alphabet-level activities, hedging gains (losses) \\nrelated to revenue were $669 million  and $150 million  for the three months ended December 31, 2022  and 2023 , \\nrespectively. For the quarter ended December 31, 2023, Alphabet-level activities included charges related to the reduction \\nin force and our office space optimization efforts totaling $1.2 billion and $62 million in accelerated rent and accelerated \\ndepreciation.\\nAdditional Information Relating to the Quarter Ended December 31, 2023  (unaudited)\\nReductions in Our Workforce and Office Space\\nIn January 2023, we announced a reduction of our workforce, and as a result we recorded employee severance and \\nrelated charges of  $2.1 billion  for the twelve months ended December 31, 2023 . In addition, we are taking actions', metadata={'source': 'data/google_earning_call.pdf', 'page': 1}), Document(page_content='Q4 2023  Supplemental Information  (in millions, except for number of employees; unaudited)\\nRevenues, Traffic Acquisition Costs (TAC), and Number of Employees\\nQuarter Ended December 31,\\n2022 2023\\nGoogle Search & other $ 42,604 $ 48,020 \\nYouTube ads  7,963  9,200 \\nGoogle Network  8,475  8,297 \\nGoogle advertising  59,042  65,517 \\nGoogle subscriptions, platforms, and devices(1) 8,796  10,794 \\nGoogle Services total  67,838  76,311 \\nGoogle Cloud  7,315  9,192 \\nOther Bets  226  657 \\nHedging gains (losses)  669  150 \\nTotal revenues $ 76,048 $ 86,310 \\nTotal TAC $ 12,925 $ 13,986 \\nNumber of employees  190,234  182,502 \\n(1) Formerly “Google other.”\\nSegment Operating Results\\nQuarter Ended December 31,\\n2022 2023\\nOperating income (loss):\\nGoogle Services $ 20,222 $ 26,730 \\nGoogle Cloud  (186)  864 \\nOther Bets  (1,237)  (863) \\nAlphabet-level activities(1) (639)  (3,034) \\nTotal income from operations $ 18,160 $ 23,697', metadata={'source': 'data/google_earning_call.pdf', 'page': 1}), Document(page_content='Segment Results\\nThe following table presents our segment revenues and operating income (loss) (in millions; unaudited):\\nQuarter Ended December 31,\\n2022 2023\\nRevenues:\\nGoogle Services $ 67,838 $ 76,311 \\nGoogle Cloud  7,315  9,192 \\nOther Bets  226  657 \\nHedging gains (losses)  669  150 \\nTotal revenues $ 76,048 $ 86,310 \\nOperating income (loss):\\nGoogle Services $ 20,222 $ 26,730 \\nGoogle Cloud  (186)  864 \\nOther Bets  (1,237)  (863) \\nAlphabet-level activities  (639)  (3,034) \\nTotal income from operations $ 18,160 $ 23,697 \\nWe report our segment results as Google Services, Google Cloud, and Other Bets:\\n•Google Services includes products and services such as ads, Android, Chrome, devices, Google Maps, \\nGoogle Play, Search, and YouTube. Google Services generates revenues primarily from advertising; fees \\nreceived for consumer subscription-based products such as YouTube TV, YouTube Music and Premium, \\nand NFL Sunday Ticket; the sale of apps and in-app purchases and devices.', metadata={'source': 'data/google_earning_call.pdf', 'page': 7})]\u001b[0m\u001b[32;1m\u001b[1;3mThe effect of staff cuts on Google in terms of severance costs resulted in employee severance and related charges of $2.1 billion for the twelve months ended December 31, 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The effect of staff cuts on Google in terms of severance costs resulted in employee severance and related charges of $2.1 billion for the twelve months ended December 31, 2023.\n"
     ]
    }
   ],
   "source": [
    "# Instance of the retriver tool being called.\n",
    "print(infer(\"Can you tell me what was the effect of staff cuts on Google in terms of severance costs?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "117f7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retriever_tool` with `{'query': 'Tesla operating income Q3'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='S U M M  A R Y H I G H L I G H T S\\n(1) Excludes SBC (stock -based compensation) and release of valuation allowance on deferred tax assets; (2) Free \\ncash flow = operating cash flow less capex; (3) Includes cash, cash equivalents and investments; (4) Tesla estimate \\nbased on preliminary data; (5) Calculated by dividing Cost of Automotive Sales Revenue by respective quarter’s \\nnew deliveries (ex -operating leases); (6)Active driver supervision required; does not make the vehicle autonomous.Profitability $8.9B GAAP operating income in 2023; $2.1B in Q4\\n$15.0B GAAP net income in 2023; $7.9B in Q4\\n$10.9B non -GAAP net income1 in 2023; $2.5B in Q4\\nOne-time non- cash tax benefit of $5.9B recorded in Q4 for the\\nrelease of valuation allowance on certain deferred tax assetsIn 2023, we delivered over 1.2 million Model Ys, making it the best -selling vehicle, \\nof any kind, globally. For a long time, many doubted the viability of EVs. Today, the \\nbest-selling vehicle on the planet is an EV.', metadata={'source': 'data/tesla_earning_call.pdf', 'page': 2}), Document(page_content='F I N A N C I A L S U M M A R Y\\n6Revenue Total revenue grew 3% YoY in Q4 to $25.2B. YoY, revenue was impacted by the following items:\\n+ growth in vehicle deliveries\\n+ growth in other parts of the business\\n+ positive FX impact of $0.1B1\\n- reduced vehicle average selling price (ASP) YoY (excl. FX impact), including unfavorable impact of mix\\n- lower FSD revenue recognition YoY due to FSD Beta wide release in North America in Q4`22\\nProfitability Our operating income decreased YoY to $2.1B in Q4, resulting in an 8.2% operating margin. YoY, operating income was \\nprimarily impacted by the following items:\\n- reduced vehicle ASP due to pricing and mix\\n- increase in operating expenses partly driven by AI and other R&D projects\\n- lower FSD revenue recognition YoY due to FSD Beta wide release in North America in Q4`22\\n- cost of Cybertruck production ramp\\n+ lower cost per vehicle, including lower raw material costs, logistics costs and IRA credit benefit \\n+ growth in vehicle deliveries', metadata={'source': 'data/tesla_earning_call.pdf', 'page': 5}), Document(page_content='Q4-2022 Q1-2023 Q2-2023 Q3-2023 Q4-2023 YoY\\nModel 3/Y production 419,088 421,371 460,211 416,800 476,777 14%\\nOther models production 20,613 19,437 19,489 13,688 18,212 -12%\\nTotal production 439,701 440,808 479,700 430,488 494,989 13%\\nModel 3/Y deliveries 388,131 412,180 446,915 419,074 461,538 19%\\nOther models deliveries 17,147 10,695 19,225 15,985 22,969 34%\\nTotal deliveries 405,278 422,875 466,140 435,059 484,507 20%\\n of which subject to operating lease accounting 15,184 22,357 21,883 17,423 10,563 -30%\\nTotal end of quarter operating lease vehicle count 140,667 153,988 168,058 176,231 176,564 26%\\nGlobal vehicle inventory (days of supply )(1)13 15 16 16 15 15%\\nSolar deployed (MW) 100 67 66 49 41 -59%\\nStorage deployed (MWh) 2,462 3,889 3,653 3,980 3,202 30%\\nTesla locations 963 1,000 1,068 1,129 1,208 25%\\nMobile service fleet 1,584 1,692 1,769 1,846 1,909 21%\\nSupercharger stations 4,678 4,947 5,265 5,595 5,952 27%\\nSupercharger connectors 42,419 45,169 48,082 51,105 54,892 29%', metadata={'source': 'data/tesla_earning_call.pdf', 'page': 6}), Document(page_content='F I N A N C I A L   S U M M  A R Y\\n(Unaudited)\\n($ in millions, except percentages and per share data) Q4-2022 Q1-2023 Q2-2023 Q3-2023 Q4-2023 YoY\\nTotal automotive revenues 21,307 19,963 21,268 19,625 21,563 1%\\nEnergy generation and storage revenue 1,310 1,529 1,509 1,559 1,438 10%\\nServices and other revenue 1,701 1,837 2,150 2,166 2,166 27%\\nTotal revenues 24,318 23,329 24,927 23,350 25,167 3%\\nTotal gross profit 5,777 4,511 4,533 4,178 4,438 -23%\\nTotal GAAP gross margin 23.8% 19.3% 18.2% 17.9% 17.6% -612 bp\\nOperating expenses 1,876 1,847 2,134 2,414 2,374 27%\\nIncome from operations 3,901 2,664 2,399 1,764 2,064 -47%\\nOperating margin 16.0% 11.4% 9.6% 7.6% 8.2% -784 bp\\nAdjusted EBITDA 5,404 4,267 4,653 3,758 3,953 -27%\\nAdjusted EBITDA margin 22.2% 18.3% 18.7% 16.1% 15.7% -652 bp\\nNet income attributable to common stockholders (GAAP) 3,687 2,513 2,703 1,853 7,928 115%\\nNet income attributable to common stockholders (non- GAAP) 4,106 2,931 3,148 2,318 2,485 -39%', metadata={'source': 'data/tesla_earning_call.pdf', 'page': 3})]\u001b[0m\u001b[32;1m\u001b[1;3mThe operating income for Tesla in Q3 was $2.1 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The operating income for Tesla in Q3 was $2.1 billion.\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"What was the operating income in Q3 for tesla?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f63b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retriever_tool` with `{'query': 'stupid backoff method'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='showcases the authors’ ingenuity in addressing\\nefficiency and overhead concerns which is a major\\nstrength of the paper.\\nSecondly, the authors’ ability to clearly artic-\\nulate the challenges they faced when training a\\nmodel at this scale (up to a trillion tokens) sets the\\nstage perfectly for their proposed Stupid Backoff\\nmethod. For example, the number of additional\\nsteps required for the distributed training of the\\nKneser-Ney algorithm (Table 1, page 5) already\\nmakes it a daunting option compared to the much\\nsimpler Stupid Backoff method.\\nThirdly, the authors’ ability to go beyond just\\nstating facts or equations adds more meaning to\\ntheir work. We would like to cite two distinct ex-\\namples of this:\\n•When presenting the stupid backoff model for-\\nmally (Brants et al., 2007) (equation 5, page\\n2), the authors articulate that this equation has\\nshifted the paradigm outside the domain of\\nlegal probabilities due to the lack of any nor-\\nmalization scheme. Furthermore, they explain', metadata={'source': 'data/G04_RA1.pdf', 'page': 1}), Document(page_content='tributed systems to generate translations. However,\\nthe authors propose an alternative strategy where\\nthey only have to contact one system. This opti-\\nmization is achieved by maintaining pre-calculated\\nprobabilities for various phrases rather than rely-\\ning on raw data. Their approach demonstrates an\\nefficiency advantage.\\n4.2 Stupid Backoff\\nStupid backoff does not generate probabilities but\\nrather scores. This could be a potential limitation\\nbecause many applications using language models\\nrely on normalized probabilities to make decisions.\\nWe suggest the authors discuss this as a limitation\\nand propose a workaround for users who want to\\nuse probabilities rather than scores.\\n4.3 Generation of n-grams\\nThe text mentions that sharding based solely on\\nthe first word of n-grams can lead to imbalanced\\nshards, impacting runtime. The authors need to\\nmention this as a potential limitation and elaborate\\non how they fixed it (using the first two words for\\nhashing).\\n4.4 Distributed Application', metadata={'source': 'data/G04_RA1.pdf', 'page': 2}), Document(page_content='it. Although the task of evaluating language\\nmodels is an entirely different research niche,\\nit would have added immensely to the paper\\nhad the authors dedicated a subsection to\\ndescribe the massive challenge that language\\nmodel evaluation actually is rather than just\\nciting shortcomings of Perplexity and sticking\\nwith it.\\nDespite these weaknesses, the significance of\\nthe paper is undeniable. The ability of Stupid\\nBackoff to outperform the SOTA Kneser-\\nNey smoothing alluded to the future of NLP\\nand machine learning in general where large\\nmodels and even larger datasets would reign\\nsupreme. The AlexNet paper (Krizhevsky\\net al., 2012) would also use a distributed train-\\ning protocol and a cheaper activation (ReLU)\\nto revolutionize Computer Vision and kick-\\nstart the deep learning era as we know it.\\n3 A List of Pressing Questions\\nUpon reading the paper, a myriad questions came\\nto us, answering which we think would lead to\\ngreater inroads in the topic and perhaps enlighten', metadata={'source': 'data/G04_RA1.pdf', 'page': 1}), Document(page_content='Large Language Models in Machine Translation - A Critical Review\\nG04 - RA 1\\n1 Summary\\nThis paper addresses the central problem of\\nmachine translation in the context of language\\nmodels; given any source language text, produce its\\ntranslation in a target language, such as French to\\nEnglish. The paper primarily discusses scalability\\nissues in n-gram language models (both model and\\ndata size) and proposes mechanisms to deal with\\nthem.\\nOne issue is the difficulty of computing the\\nprobabilities for smoothing, for which the paper\\nintroduces a new mechanism, ‘Stupid Backoff’.\\nThe main advantage of this is that it does not\\nuse any discounting methods but instead uses\\nthe counts of the tokens. This allows us to use a\\ndistributed training environment to compute the\\nprobabilities, which is both cheaper and produces\\nsimilar results as Kneser-Ney smoothing when\\ndealing with large data sets.\\nThe paper uses the MapReduce programming\\nmodel to generate language models based on the', metadata={'source': 'data/G04_RA1.pdf', 'page': 0})]\u001b[0m\u001b[32;1m\u001b[1;3mThe Stupid Backoff method does not generate probabilities but rather scores, which could be a potential limitation for many applications using language models that rely on normalized probabilities to make decisions. It is suggested that the authors discuss this limitation and propose a workaround for users who want to use probabilities rather than scores.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Stupid Backoff method does not generate probabilities but rather scores, which could be a potential limitation for many applications using language models that rely on normalized probabilities to make decisions. It is suggested that the authors discuss this limitation and propose a workaround for users who want to use probabilities rather than scores.\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"Was the stupid backoff method any good?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b02fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNO RESULTS FOUND. WANT ME TO SEARCH THE WEB?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "NO RESULTS FOUND. WANT ME TO SEARCH THE WEB?\n"
     ]
    }
   ],
   "source": [
    "print(infer('why is the sky blue?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5886bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `google_search` with `why is the sky blue`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mWhy Is the Sky Blue? The Short Answer: Sunlight reaches Earth's atmosphere and is scattered in all directions by all the gases and particles in the air. Blue ... Nov 4, 2023 ... The sky's blueness isn't from reflecting the water. Instead, its color has to do with scattered light. Gases and particles in Earth's atmosphere scatter sunlight in all directions. Blue light is scattered more than other colors because it travels as shorter, ... Why is the sky blue? ... As white light passes through our atmosphere, tiny air molecules cause it to 'scatter'. The scattering caused by these tiny air molecules ... Why is the sky blue? A clear cloudless day-time sky is blue because molecules in the air scatter blue light from the Sun more than they scatter red light. When ... Jan 31, 2020 ... The sky is blue due to a phenomenon called Raleigh scattering. This scattering refers to the scattering of electromagnetic radiation (of ... Jun 9, 2023 ... basically you are mostly right. The scattering of blue or red and what you see depends on the path of light thru atmosphere. When the sun is ... Watch the video to discover the answer to \"Why is the sky blue?\" and don't forget to vote for next week's question! Jul 18, 2011 ... When light enters the atmosphere it hits these small molecules such as oxygen (21%) and nitrogen (78%) (gases). Gases then tend to absorb not ... Nov 5, 2015 ... Since the sky was always the color of the sun's rays, the splattered paint gave it an odd polka-dot look, which the artist found unappealing, so ...\u001b[0m\u001b[32;1m\u001b[1;3mThe sky appears blue because sunlight reaches Earth's atmosphere and is scattered in all directions by gases and particles in the air. Blue light is scattered more than other colors because it travels as shorter wavelengths.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The sky appears blue because sunlight reaches Earth's atmosphere and is scattered in all directions by gases and particles in the air. Blue light is scattered more than other colors because it travels as shorter wavelengths.\n"
     ]
    }
   ],
   "source": [
    "print(infer('yes'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
